<!DOCTYPE html>
<html lang="en">
  <!-- Head tag -->
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Title -->
  
  <title>python异步 - Cuddleidiots</title>

  <!--Favicon-->
  <link rel="icon" href="favicon/favicon.ico">

  <!--Description-->
  
      <meta name="description" content="事情是这样的，有一天，小刘看到豆瓣，恰好小刘很勤（wu）奋（liao），秉承只要任何事情花费时间超过90秒，一定会想写一个脚本自动化实现的不严肃人生观，心血来潮的她决定写个程序，把豆瓣Top250的电影列表给爬下来，于是她欣喜地开始写，不到十分钟，小刘就写好了第一个程序。
#-*- coding:u">
  

  <!--Author-->
  
      <meta name="author" content="631">
  

  <!-- Pure CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
  <link href="https://fonts.googleapis.com/css?family=Crimson+Text|Open+Sans:300,800" rel="stylesheet">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/styles.css">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->

  <!-- Google Analytics -->
  

</head>
 <!-- highlight.js代码高亮主题 css 引入-->
  <link rel="stylesheet" href="/plugins/highlight/styles/Darcula.css">
  <!-- highlight.js代码高亮主题 css 引入-->

  <body>
  	<div class="container-fluid navbar-container m-sm-5">
      <!-- Header -->
      <nav class="navbar navbar-toggleable-sm navbar-light px-1 py-3 my-3 mb-sm-5">
  <a class="navbar-brand ml-2" href="/">Cuddleidiots</a>
  <button class="navbar-toggler navbar-toggler-right py-2" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse text-center" id="navbarCollapse">
    <ul class="navbar-nav ml-auto my-auto">
      
        <li class="nav-item">
          <a class="nav-link" href="/about">About</a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="/contact">Contact</a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="https://instagram.com/hey631">Instagram</a>
        </li>
      
        <li class="nav-item">
          <a class="nav-link" href="https://weibo.com/mosican/">Weibo</a>
        </li>
      
    </ul>
    <hr class="hidden-md-up" />
  </div>
</nav>


  		<div class="row">
  			<div class="col-12 mb-4">
  <img class="img-fluid project-img" src="/images/room.jpg" alt="python异步">
</div>
<div class="col-lg-4 col-12 pt-3 px-4 pr-lg-5">
  <h1>python异步</h1>
</div>
<div class="col-lg-8 col-12 pt-lg-3 mb-4 pl-lg-5 px-lg-0 px-4 portfolio-content">
  <p>事情是这样的，有一天，小刘看到豆瓣，恰好小刘很勤（wu）奋（liao），秉承<strong>只要任何事情花费时间超过90秒，一定会想写一个脚本自动化实现</strong>的不严肃人生观，心血来潮的她决定写个程序，把豆瓣Top250的电影列表给爬下来，于是她欣喜地开始写，不到十分钟，小刘就写好了第一个程序。</p>
<pre><code class="python">#-*- coding:utf-8 -*-
import urllib.request
import ssl
from lxml import etree

url = &#39;https://movie.douban.com/top250&#39;
context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_1)

def fetch_page(url):
    response = urllib.request.urlopen(url, context=context)
    return response

def parse(url):
    response = fetch_page(url)
    page = response.read()
    html = etree.HTML(page)

    xpath_movie = &#39;//*[@id=&quot;content&quot;]/div/div[1]/ol/li&#39;
    xpath_title = &#39;.//span[@class=&quot;title&quot;]&#39;
    xpath_pages = &#39;//*[@id=&quot;content&quot;]/div/div[1]/div[2]/a&#39;

    pages = html.xpath(xpath_pages)
    fetch_list = []
    result = []

    for element_movie in html.xpath(xpath_movie):
        result.append(element_movie)

    for p in pages:
        fetch_list.append(url + p.get(&#39;href&#39;))

    for url in fetch_list:
        response = fetch_page(url)
        page = response.read()
        html = etree.HTML(page)
        for element_movie in html.xpath(xpath_movie):
            result.append(element_movie)

    for i, movie in enumerate(result, 1):
        title = movie.find(xpath_title).text         
        print(i, title)

def main():
    parse(url)

if __name__ == &#39;__main__&#39;:
    main()
</code></pre>
<p>程序也不出意外地正常运行。</p>
<p> <img src="/images/one.png" height="95%" width="95%"></p>
<p>但是，这个程序异常的慢，有多慢呢？小刘在主函数中加了下面一段代码。</p>
<pre><code class="python">def main():
    start = time()    
    for i in range(5):
        parse(url)
    end = time()
    print (&#39;Cost {} seconds&#39;.format((end - start) / 5))
</code></pre>
<p>发现总共耗时7.6秒！！</p>
<p> <img src="/images/two.png" height="95%" width="95%"></p>
<p>小刘不禁陷入了沉思…</p>
<p>小刘突然想起了一个库，叫<strong>requests</strong>，比那urllib,urllib2,urllib3,urllibn…不知高到哪里去了！小刘修改程序，用requests代替了标准库urllib。</p>
<pre><code class="python">import requests
from lxml import etree
from time import time

url = &#39;https://movie.douban.com/top250&#39;

def fetch_page(url):
    response = requests.get(url)
    return response

def parse(url):
    response = fetch_page(url)
    page = response.content
    html = etree.HTML(page)

    xpath_movie = &#39;//*[@id=&quot;content&quot;]/div/div[1]/ol/li&#39;
    xpath_title = &#39;.//span[@class=&quot;title&quot;]&#39;
    xpath_pages = &#39;//*[@id=&quot;content&quot;]/div/div[1]/div[2]/a&#39;

    pages = html.xpath(xpath_pages)
    fetch_list = []
    result = []

    for element_movie in html.xpath(xpath_movie):
        result.append(element_movie)

    for p in pages:
        fetch_list.append(url + p.get(&#39;href&#39;))

    for url in fetch_list:
        response = fetch_page(url)
        page = response.content
        html = etree.HTML(page)
        for element_movie in html.xpath(xpath_movie):
            result.append(element_movie)

    for i, movie in enumerate(result, 1):
        title = movie.find(xpath_title).text
        # print(i, title)
</code></pre>
<p>结果一测，6.5秒。虽然比用urllib快了1秒多，但是总体来说，他们基本还是处于同一水平线的，程序并没有快很多，这一点的差距是requests对请求做了优化导致的，<strong>根据业务场景选择适当的轮子能避免踩许多的坑</strong>。</p>
<p>  <img src="/images/three.png" height="95%" width="95%"></p>
<p>小刘想：是我的程序写的太挫还是是lxml这个库解析的速度太慢了，用正则表达式会不会好一些？</p>
<p>于是把lxml库换成了标准的re库。</p>
<pre><code class="python">#-*- coding:utf-8 -*-
import requests
from time import time
import re

url = &#39;https://movie.douban.com/top250&#39;

def fetch_page(url):
    response = requests.get(url)
    return response

def parse(url):
    response = fetch_page(url)
    page = response.content

    fetch_list = set()
    result = []

    for title in re.findall(rb&#39;&lt;a href=.*\s.*&lt;span class=&quot;title&quot;&gt;(.*)&lt;/span&gt;&#39;, page):
        result.append(title)

    for postfix in re.findall(rb&#39;&lt;a href=&quot;(\?start=.*?)&quot;&#39;, page):
        fetch_list.add(url + postfix.decode())

    for url in fetch_list:
        response = fetch_page(url)
        page = response.content
        for title in re.findall(rb&#39;&lt;a href=.*\s.*&lt;span class=&quot;title&quot;&gt;(.*)&lt;/span&gt;&#39;, page):
            result.append(title)

    for i, title in enumerate(result, 1):
        title = title.decode()
        # print(i, title)
</code></pre>
<p>再一跑，咦，又足足提升了将近一秒。</p>
<p>  <img src="/images/four.png" height="95%" width="95%"></p>
<p>程序变得更短了，运行得也更快了，但这样写出来的程序虽然看起来更短了，但所做的都是在盲目地求<strong>快</strong>，但完全没有<strong>扩展性</strong>可言。虽然这样做可以满足普通的需求场景，但当程序逻辑变复杂时，依赖原生正则表达式的程序会更加难以维护，借助一些专门做这些事情的解析库，才能使程序变得清晰。其次，这种网络应用通常瓶颈都在IO层面，解决等待读写的问题比提高文本解析速度来的更有性价比。小刘想起了多进程（？），正好用他们来解决实际问题。</p>
<p>  <img src="/images/five.png" height="30%" width="30%"></p>
<p>既然时间都耗在网络IO上了，那我每一页的获取都用一个线程去处理不就好了吗，于是写了多线程版本的代码。</p>
<pre><code class="python">#-*- coding:utf-8 -*-
import requests
from lxml import etree
from time import time
from threading import Thread

url = &#39;https://movie.douban.com/top250&#39;

def fetch_page(url):
    response = requests.get(url)
    return response

def parse(url):
    response = fetch_page(url)
    page = response.content
    html = etree.HTML(page)

    xpath_movie = &#39;//*[@id=&quot;content&quot;]/div/div[1]/ol/li&#39;
    xpath_title = &#39;.//span[@class=&quot;title&quot;]&#39;
    xpath_pages = &#39;//*[@id=&quot;content&quot;]/div/div[1]/div[2]/a&#39;

    pages = html.xpath(xpath_pages)
    fetch_list = []
    result = []

    for element_movie in html.xpath(xpath_movie):
        result.append(element_movie)

    for p in pages:
        fetch_list.append(url + p.get(&#39;href&#39;))

    def fetch_content(url):
        response = fetch_page(url)
        page = response.content
        html = etree.HTML(page)
        for element_movie in html.xpath(xpath_movie):
            result.append(element_movie)

    threads = []
    for url in fetch_list:
        t = Thread(target=fetch_content, args=[url])
        t.start()
        threads.append(t)

    for t in threads:
        t.join()

    for i, movie in enumerate(result, 1):
        title = movie.find(xpath_title).text
        # print(i, title)
</code></pre>
<p>效果果然立竿见影！多线程有效的解决了阻塞等待的问题，这个程序足足比之前的程序快了80%。只需要1.4秒就可完成电影列表的抓取。</p>
<p>  <img src="/images/six.png" height="95%" width="95%"></p>
<p>还是觉得不够过瘾，既然Python的多线程也受制于GIL，为什么我不用多进程呢？于是话不多说又撸出了一个基于多进程的版本。用4个进程的进程池来并行处理网络数据。</p>
<pre><code class="python">#-*- coding:utf-8 -*-
import requests
from lxml import etree
from time import time
from concurrent.futures import ProcessPoolExecutor

url = &#39;https://movie.douban.com/top250&#39;

def fetch_page(url):
    response = requests.get(url)
    return response

def fetch_content(url):
    response = fetch_page(url)
    page = response.content
    return page

def parse(url):
    page = fetch_content(url)
    html = etree.HTML(page)

    xpath_movie = &#39;//*[@id=&quot;content&quot;]/div/div[1]/ol/li&#39;
    xpath_title = &#39;.//span[@class=&quot;title&quot;]&#39;
    xpath_pages = &#39;//*[@id=&quot;content&quot;]/div/div[1]/div[2]/a&#39;

    pages = html.xpath(xpath_pages)
    fetch_list = []
    result = []

    for element_movie in html.xpath(xpath_movie):
        result.append(element_movie)

    for p in pages:
        fetch_list.append(url + p.get(&#39;href&#39;))

    with ProcessPoolExecutor(max_workers=4) as executor:
        for page in executor.map(fetch_content, fetch_list):
            html = etree.HTML(page)
            for element_movie in html.xpath(xpath_movie):
                result.append(element_movie)

    for i, movie in enumerate(result, 1):
        title = movie.find(xpath_title).text
        # print(i, title)
</code></pre>
<p>结果是2秒，甚至还不如多线程的版本。。。</p>
<p>   <img src="/images/seven.png" height="95" width="95%"></p>
<p><em>(注：ThreadPoolExecutor和ProcessPoolExecutor是Python3.2之后引入的分别对线程池和进程池的一个封装，如果使用Python2.x，需要安装<code>futures</code>这个库才能使用它们。)</em></p>
<p>这跟预期完全不符合。。<br>多进程带来的优点（cpu处理）并没有得到体现，<strong>反而创建和调度进程带来的开销要远超出它的正面效应</strong>，拖了一把后腿。即便如此，多进程带来的效益相比于之前单进程单线程的模型要好得多。很正常的，<strong>多进程和多线程除了创建的开销大之外还有一个难以根治的缺陷，就是处理进程之间或线程之间的协作问题，因为是依赖多进程和多线程的程序在不加锁的情况下通常是不可控的，而协程则可以完美地解决协作问题，由用户来决定协程之间的调度。</strong></p>
<p>然后小刘用了基于协程的网络库<code>gevent</code>，据说用了gevent的猴子补丁后，整个程序就会变成异步的了。？</p>
<p>迫不及待地要看看效果，马上写出了基于gevent的栗子：</p>
<pre><code class="python">#-*- coding:utf-8 -*-
import requests
from lxml import etree
from time import time
import gevent
from gevent import monkey
monkey.patch_all()

url = &#39;https://movie.douban.com/top250&#39;

def fetch_page(url):
    response = requests.get(url)
    return response

def fetch_content(url):
    response = fetch_page(url)
    page = response.content
    return page

def parse(url):
    page = fetch_content(url)
    html = etree.HTML(page)

    xpath_movie = &#39;//*[@id=&quot;content&quot;]/div/div[1]/ol/li&#39;
    xpath_title = &#39;.//span[@class=&quot;title&quot;]&#39;
    xpath_pages = &#39;//*[@id=&quot;content&quot;]/div/div[1]/div[2]/a&#39;

    pages = html.xpath(xpath_pages)
    fetch_list = []
    result = []

    for element_movie in html.xpath(xpath_movie):
        result.append(element_movie)

    for p in pages:
        fetch_list.append(url + p.get(&#39;href&#39;))

    jobs = [gevent.spawn(fetch_content, url) for url in fetch_list]
    gevent.joinall(jobs)
    [job.value for job in jobs]

    for page in [job.value for job in jobs]:
        html = etree.HTML(page)
        for element_movie in html.xpath(xpath_movie):
            result.append(element_movie)

    for i, movie in enumerate(result, 1):
        title = movie.find(xpath_title).text
        # print(i, title)
</code></pre>
<p>只有1.2秒，果然很快。而且我们看整个程序，几乎看不到有异步处理的影子，<strong>gevent给予了我们一种以同步逻辑来书写异步程序的能力</strong>，看<code>monkey.patch_all()</code>这段代码，它是整个程序实现异步的中核，当我们给程序打了猴子补丁后，Python程序在运行时会动态地将一些网络库（例如socket，thread）替换掉，变成异步的库。使得程序在进行网络操作的时候都变成异步的方式去工作，效率就自然提升很多了。</p>
<p>  <img src="/images/eight.png" height="95%" width="95%"></p>
<p>虽然程序变得很快了，gevent给小刘带来了一定困惑，gevent这玩意心目中Pythonic的清晰优雅还是有距离的。Python社区显然也意识到Python需要一个独立的标准库来支持协程，于是就有了后来的asyncio。</p>
<p>小刘把同步的requests库改成了支持asyncio的<code>aiohttp</code>库，使用3.5的async/await语法（<em>3.5之前用@asyncio.coroutine和yield from代替</em>）写出了协程版本的例子。</p>
<pre><code class="python">#-*- coding:utf-8 -*-
from lxml import etree
from time import time
import asyncio
import aiohttp

url = &#39;https://movie.douban.com/top250&#39;

async def fetch_content(url):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.text()
    return &#39;&#39;

async def parse(url):
    page = await fetch_content(url)
    html = etree.HTML(page)

    xpath_movie = &#39;//*[@id=&quot;content&quot;]/div/div[1]/ol/li&#39;
    xpath_title = &#39;.//span[@class=&quot;title&quot;]&#39;
    xpath_pages = &#39;//*[@id=&quot;content&quot;]/div/div[1]/div[2]/a&#39;

    pages = html.xpath(xpath_pages)
    fetch_list = []
    result = []

    for element_movie in html.xpath(xpath_movie):
        result.append(element_movie)

    for p in pages:
        fetch_list.append(url + p.get(&#39;href&#39;))

    tasks = [fetch_content(url) for url in fetch_list]
    pages = await asyncio.gather(*tasks)

    for page in pages:
        html = etree.HTML(page)
        for element_movie in html.xpath(xpath_movie):
            result.append(element_movie)

    for i, movie in enumerate(result, 1):
        title = movie.find(xpath_title).text
        # print(i, title)

def main():
    loop = asyncio.get_event_loop()    
    start = time()    
    for i in range(5):
        loop.run_until_complete(parse(url))
    end = time()
    print (&#39;Cost {} seconds&#39;.format((end - start) / 5))
    loop.close()
</code></pre>
<p>1.7秒，也不错。用上了async/await语法使得程序的可读性提高了不少。这个协程如何运作，很容易理解，总个有10个请求，除了第1个请求是要等待之外（先要抓取分页），剩余9个请求都是协作式的，由Event Loop进行事件调度，试想一下有9个人去仓库拿东西，但要通过咨询仓库管理员来获取货物，仓库里有许多工人，现在如果是同步模型，会是这样的场景：小一，小二，小三…去仓库排队，小一问管理员要货物，管理员通知工人去仓库搬运小一需要的东西，小一就在那里等待，直到取到自己的东西，离开，轮到小二，小二进行同样的操作…而如果是协作模型，则会是这样的场景：小一，小二，小三…去仓库排队，小一问管理员要货物，管理员通知工人去仓库搬运小一需要的东西，管理员对小一说：你的需要等一会儿，去后面等一下，现在轮到小二了。然后小一就排到队伍的最后面，小二问管理员后，管理员对小二说：你的需要等一会儿，去后面等一下，现在轮到小三了，然后小二也排到队伍的最后面，接着每个人都执行一样的操作…直到小九离开，轮到小一了，这时候小A询问管理员他的东西取好没有，这时候又分两种情况：如果取好了，小A领取货物离开，还没取好，小A继续返回到队伍的最后面，以此类推，直至所有人都领取完自己的货物。试想一下，如果每个人平均等待领取货物的时间需要10秒，离开的时间需要1秒，那么同步模型的总耗时最少需要99秒，而协作模型最少只需要同步模型的1/3时间。在这个例子中，小一，小二..小九对应了每个分页的请求，他们都是被aiohttp进行异步封装的，管理员则扮演了Event Loop，类似一个协调者的角色，工人的搬运过程则类似于HTTP协议的传输过程，当然，这些只是比喻，实际上过程要复杂的多。（我话好多啊。。。。。）</p>
<p>  <img src="/images/nine.png" height="95%" width="95%"></p>
<p>异步方式有很多，这里列出了比较常见的几种，在实际使用中，应该根据使用场景来挑选最合适的应用方案，影响程序效率的因素有很多，以上不同的异步方式在不同的场景下也会有不一样的表现，不要抱死在一个大树上，该用同步的地方用同步，该用异步的地方异步，这样才能构建出更加灵活的网络应用。</p>
<p>End</p>
<hr>
<p>如果你是小刘，你会有更好的方案吗？欢迎讨论么么叽。</p>

</div>


      </div>
      
  	</div>

    <!-- After footer scripts -->
    <script src="https://code.jquery.com/jquery-3.1.1.slim.min.js" integrity="sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js" integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
  <!-- highlight.js代码高亮主题 script 引入-->
  <script src="/plugins/highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- highlight.js代码高亮主题 script 引入-->
  </body>
</html>
